{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2493c14a-9f5d-44a4-9a31-96fb5de0449c",
    "_uuid": "37a8f535fe48bbf68ed4bc30d2cfa78a4979df68"
   },
   "source": [
    "Here I describe how I got from around 2000th place to 10th. As there are already many great kernels, I will try to make that description short. \n",
    "\n",
    "First I took all the data which was there, threw it into random forest and got 0.13825 on the leaderboard. Next I describe how I improved that... \n",
    "\n",
    "**Step 0: log-transform of sales.** Log-transform of the dependent variable. There are two reasons to predict log of sales, not sales: first, evaluation metric depends on log of sales and, second, many monetary entities have distribution close to log-normal, i.e. heavy-tailed. Usually it is better to predict smt which is not heavy tailed (I honestly tried to predict sales per se, and results were worse).   \n",
    "\n",
    "**Step 1: re-coding predictors / missings / outliers.** Some of the predictors which are ordinal (i.e. there is a natural order in their values) initially are stored as factors. I re-coded them in a sensible way (there are many great kernels which describe in detail how to do it; examples of such predictors are *ExterQual* and *BsmtCond*). Obviously I had to fill in missing values - missings in numeric predictors I filled in with zeros when it made sense (e.g. if there are no baths in the basement, BsmtFullBath should be equal to zero), missings in categorical predictors I filled in with modes or other typical values. Last, many users advise removing observations with outliers in predictors *OverallQual* and *GrLivArea* - it is worth doing.\n",
    "\n",
    "**Step 2: non-linearities.** As I planned to use linear methods (lasso, ridge, svm), I *replaced* all heavy-tailed predictors with their logs and for some of the predictors *added* their squares (i.e. we have predictor X and we add predictor X^2). Replacing heavy-tailed predictors with their logs is motivated by the fact that a) linear methods might fit such predictors with very small weights and most of the information contained in the values might be lost b) predictions when such predictors take very high values might be also very high or misleading. Adding squares is motivated by non-linearities in scatterplots \"predictor vs. log of sales\" - we assume that similar non-linearities will also hold when we add predictor to multi-dimensional model). Check, for example that scatterplot of BsmtQual vs log of Sales:\n",
    "\n",
    "![BsmtQual](https://i.imgur.com/K2HLZtbm.png)\n",
    "\n",
    "**Step 3: new predictors.** For some of the predictors helped adding indicator variables which are equal to one if respective predictor take certain value. I chose predictors and their values to indicate based on scatterplots - if on scatterplot \"predictor vs. log of sales\" for the certain value of the predictor there is a spike in values of log-sales or non-linearity, such value of predictor is a good candidate. Example of such non-linearity can be found below. Scatterlot shows re-coded values of BsmtFinType1 vs. log of sales. In the model I added indicator which is equal to one when BsmtFinType1 = 1 (which corresponds to BsmtFinType1 equal to *Unf*):\n",
    "\n",
    "![BsmtFinType1](https://i.imgur.com/MCosvgDm.png)\n",
    "\n",
    "**Step 4: stacking.** As a next  step I used 10-fold cross-validation and stacking: for each \"run\" of cross-validation, I fit 5 models on 9 out of 10 folds (lasso, ridge, elastic net, GBM and LGB), make predictions on the left-out fold and use these five sets of predictions as an input into another lasso model to forecast log of sales on that left-out fold (such lasso model is called meta-model). In total we have 6\\*10=60 models - 10 sets of 6 models. All these models I used to make final predictions: we take test dataset, make predictions using 5 sub-models and then use outputs of these models as an input into respective meta-model to get set of predictions for given set of models; we repeat that process 10 times to get 10 sets of predictions and then average them using arithmetic mean to get data used for submission. Steps 0-4 gave me smt around 0.120 on the LB. \n",
    "\n",
    "**Step 5: tuning.** For stacking I used 6 different models and each needed tuning (for each \"run\" of cross-validation I fit 6 models with always the same parameters, e.g. we also fit ridge with regularization parameter equal to 9.0). I spent a lot of time and submissions to fine-tune parameters (best improvement was by tuning *min_samples_leaf* and *min_samples_split* for GradientBoostingRegressor). Eventually I got smt around 0.1188 and was confident I won't be able to improve it by tuning.\n",
    "\n",
    "**Step 6: more with missing values.** Next I tried different strategies of filling in missing values (modes/means/medians/etc.). Best thing which worked was to some extent unexpected: it was based on mice package in R and is described here - https://www.kaggle.com/couyang/svm-benchmark-approach-0-11820-lb-top-13. First I tried using dataset I got from R/mice in my python code for stacking, then I tried pre-filling in some missing values in sensible way and using mice on top, but smt else gave best results. I took training dataset without any transformations or cleaning, used mice on top and then fitted SVM. Basically, I could just re-use code by https://www.kaggle.com/couyang per se - no outlier cleaning, no feature transformations, just mice and svm, nothing else. Using R/mice/svm and python/stacking and taking geometric mean of these predictions gave me 0.11505 on LB, which was already great. It was unexpected that such simple and not-business-driven approach to fill in outliers worked. Note, that mice might fill in missings in numeric predictors with smt not equal to zero when there should be zero. I suppose that such approach helped me as it was quite different from what I got in python.\n",
    "\n",
    "**Step 7: brutal force.** I was already happy with 0.11505 but there was last thing I wanted to try. Regression often works bad for edge cases - for small or big values of predictors. I took train, ran R/mice/svm, python/stacking, averaged results using geometric mean, got final predictions for log of sales and plotted them against logs of real values: \n",
    "\n",
    "![scatter](https://i.imgur.com/5Ky8u7pm.png)\n",
    "\n",
    "It was obvious from that picture that for small final predictions we are overestimating sales, for big values of predictions we are underestimating. I tried fitting splines (natural/smoothing) and local regression in R and using them on a test set. It immediately gave me improvement. Final and best submit was based on smt which is quite brutal: we take predicted sales (not log-, but sales as they are), take top and bottom percentiles and manually increase/decrease forecasts. Which eventually gave me 0.10943 on the LB. Also, obviously brute force approach of manual change in forecasted values is more overfitting then anything else as there is no so much business motivation behind it (even using splines would be better for productionized model). It is important to note that such brute force approach improves forecasts for small number of observations, but it seems that is is enough to improve score on LB.\n",
    "\n",
    "Some other things I tried which didn't help include a) PCA  b) adding more and more predictors c) adding random forest, KRR, SVM, xgboost to stacking d) using keras on tensorflow and scaling + dense neural nets.\n",
    "\n",
    "**Summary:** I would say that productionized version would have evaluation metric of around 0.12, as it is possible to build simple model which will give such result. Interesting things which helped: adding squares of some of the predictors, using mice+svm in R on top of the raw data, working with edge-predictions (very low of very high ones).\n",
    "\n",
    "**NB** It seems that kaggle does not support having both R and python inside one notebook, so I had to comment R code. Also I use stacking on 30-fold cross-validation (it takes around 10 minutes on my laptop to fit all models), but here it gets stopped due to timeout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_cell_guid": "adc8ac91-61cf-4051-b795-1bfc5e2c0f33",
    "_kg_hide-input": false,
    "_uuid": "6e54aeebd72b22c2e44db1d50e14159935957364",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet \n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_cell_guid": "845d90d3-0971-4a24-9fa6-92cb22b19cdf",
    "_uuid": "3b89d97eb0c72f5063119fb06231600ce693d87c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------ Reading and cleaning data ------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_cell_guid": "1bbaacc6-3988-478d-b775-817a0798c18d",
    "_uuid": "36665dd3a796256264d5a16e935586f2913c8e89",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('train.csv')\n",
    "\n",
    "# As suggested by many participants, we remove several outliers\n",
    "mydata.drop(mydata[(mydata['OverallQual']<5) & (mydata['SalePrice']>200000)].index, inplace=True)\n",
    "mydata.drop(mydata[(mydata['GrLivArea']>4000) & (mydata['SalePrice']<300000)].index, inplace=True)\n",
    "mydata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Some of the non-numeric predictors are stored as numbers; we convert them into strings \n",
    "mydata['MSSubClass'] = mydata['MSSubClass'].apply(str)\n",
    "mydata['YrSold'] = mydata['YrSold'].astype(str)\n",
    "mydata['MoSold'] = mydata['MoSold'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_cell_guid": "79ad9518-7418-4393-922b-04b7b0a6e720",
    "_uuid": "b444443b41033b9f35c7e159b33ffcc29afb2879",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------ Function to fill in missings ------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "_cell_guid": "e67dabbe-0611-4486-a179-86c125c1347c",
    "_uuid": "1eb045c7ebf9356f312743a97f7b82c700e628bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create funtion which fills all the missing values\n",
    "# Pay attention that some of the missing values of numeric predictors first are filled in with zeros and then \n",
    "# small values are filled in with median/average (and indicator variables are created to account for such change: \n",
    "# for each variable we create  which are equal to one);\n",
    "\n",
    "def fill_missings(res):\n",
    "\n",
    "    res['Alley'] = res['Alley'].fillna('missing')\n",
    "    res['PoolQC'] = res['PoolQC'].fillna(res['PoolQC'].mode()[0])\n",
    "    res['MasVnrType'] = res['MasVnrType'].fillna('None')\n",
    "    res['BsmtQual'] = res['BsmtQual'].fillna(res['BsmtQual'].mode()[0])\n",
    "    res['BsmtCond'] = res['BsmtCond'].fillna(res['BsmtCond'].mode()[0])\n",
    "    res['FireplaceQu'] = res['FireplaceQu'].fillna(res['FireplaceQu'].mode()[0])\n",
    "    res['GarageType'] = res['GarageType'].fillna('missing')\n",
    "    res['GarageFinish'] = res['GarageFinish'].fillna(res['GarageFinish'].mode()[0])\n",
    "    res['GarageQual'] = res['GarageQual'].fillna(res['GarageQual'].mode()[0])\n",
    "    res['GarageCond'] = res['GarageCond'].fillna('missing')\n",
    "    res['Fence'] = res['Fence'].fillna('missing')\n",
    "    res['Street'] = res['Street'].fillna('missing')\n",
    "    res['LotShape'] = res['LotShape'].fillna('missing')\n",
    "    res['LandContour'] = res['LandContour'].fillna('missing')\n",
    "    res['BsmtExposure'] = res['BsmtExposure'].fillna(res['BsmtExposure'].mode()[0])\n",
    "    res['BsmtFinType1'] = res['BsmtFinType1'].fillna('missing')\n",
    "    res['BsmtFinType2'] = res['BsmtFinType2'].fillna('missing')\n",
    "    res['CentralAir'] = res['CentralAir'].fillna('missing')\n",
    "    res['Electrical'] = res['Electrical'].fillna(res['Electrical'].mode()[0])\n",
    "    res['MiscFeature'] = res['MiscFeature'].fillna('missing')\n",
    "    res['MSZoning'] = res['MSZoning'].fillna(res['MSZoning'].mode()[0])    \n",
    "    res['Utilities'] = res['Utilities'].fillna('missing')\n",
    "    res['Exterior1st'] = res['Exterior1st'].fillna(res['Exterior1st'].mode()[0])\n",
    "    res['Exterior2nd'] = res['Exterior2nd'].fillna(res['Exterior2nd'].mode()[0])    \n",
    "    res['KitchenQual'] = res['KitchenQual'].fillna(res['KitchenQual'].mode()[0])\n",
    "    res[\"Functional\"] = res[\"Functional\"].fillna(\"Typ\")\n",
    "    res['SaleType'] = res['SaleType'].fillna(res['SaleType'].mode()[0])\n",
    "    res['SaleCondition'] = res['SaleCondition'].fillna('missing')\n",
    "    \n",
    "    flist = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n",
    "                     'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n",
    "                     'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n",
    "                     'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n",
    "                     'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\n",
    "    for fl in flist:\n",
    "        res[fl] = res[fl].fillna(0)\n",
    "        \n",
    "    res['TotalBsmtSF'] = res['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n",
    "    res['2ndFlrSF'] = res['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n",
    "    res['GarageArea'] = res['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n",
    "    res['GarageCars'] = res['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\n",
    "    res['LotFrontage'] = res['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n",
    "    res['MasVnrArea'] = res['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n",
    "    res['BsmtFinSF1'] = res['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n",
    "    \n",
    "      \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_cell_guid": "aab8e4d2-5b00-4076-b609-ea467b13f3e0",
    "_uuid": "8a801051a96a20b92088f17d10208e596681f6bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- Filling in missing values, re-coding ordinal variables -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_cell_guid": "4b0b079c-ead3-46ad-a8b3-060f06c6ce05",
    "_uuid": "dd715d7d568ad0a53dc4574221a917d8eb7c044c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running function to fill in missings\n",
    "mydata = fill_missings(mydata)\n",
    "mydata['TotalSF'] = mydata['TotalBsmtSF'] + mydata['1stFlrSF'] + mydata['2ndFlrSF']\n",
    "\n",
    "# Working with ordinal predictors\n",
    "def QualToInt(x):\n",
    "    if(x=='Ex'):\n",
    "        r = 0\n",
    "    elif(x=='Gd'):\n",
    "        r = 1\n",
    "    elif(x=='TA'):\n",
    "        r = 2\n",
    "    elif(x=='Fa'):\n",
    "        r = 3\n",
    "    elif(x=='missing'):\n",
    "        r = 4\n",
    "    else:\n",
    "        r = 5\n",
    "    return r\n",
    "\n",
    "mydata['ExterQual'] = mydata['ExterQual'].apply(QualToInt)\n",
    "mydata['ExterCond'] = mydata['ExterCond'].apply(QualToInt)\n",
    "mydata['KitchenQual'] = mydata['KitchenQual'].apply(QualToInt)\n",
    "mydata['HeatingQC'] = mydata['HeatingQC'].apply(QualToInt)\n",
    "mydata['BsmtQual'] = mydata['BsmtQual'].apply(QualToInt)\n",
    "mydata['BsmtCond'] = mydata['BsmtCond'].apply(QualToInt)\n",
    "mydata['FireplaceQu'] = mydata['FireplaceQu'].apply(QualToInt)\n",
    "mydata['GarageQual'] = mydata['GarageQual'].apply(QualToInt)\n",
    "mydata['PoolQC'] = mydata['PoolQC'].apply(QualToInt)\n",
    "\n",
    "def SlopeToInt(x):\n",
    "    if(x=='Gtl'):\n",
    "        r = 0\n",
    "    elif(x=='Mod'):\n",
    "        r = 1\n",
    "    elif(x=='Sev'):\n",
    "        r = 2\n",
    "    else:\n",
    "        r = 3\n",
    "    return r\n",
    "\n",
    "mydata['LandSlope'] = mydata['LandSlope'].apply(SlopeToInt)\n",
    "mydata['CentralAir'] = mydata['CentralAir'].apply( lambda x: 0 if x == 'N' else 1) \n",
    "mydata['Street'] = mydata['Street'].apply( lambda x: 0 if x == 'Pave' else 1) \n",
    "mydata['PavedDrive'] = mydata['PavedDrive'].apply( lambda x: 0 if x == 'Y' else 1)\n",
    "\n",
    "def GFinishToInt(x):\n",
    "    if(x=='Fin'):\n",
    "        r = 0\n",
    "    elif(x=='RFn'):\n",
    "        r = 1\n",
    "    elif(x=='Unf'):\n",
    "        r = 2\n",
    "    else:\n",
    "        r = 3\n",
    "    return r\n",
    "\n",
    "mydata['GarageFinish'] = mydata['GarageFinish'].apply(GFinishToInt)\n",
    "\n",
    "def BsmtExposureToInt(x):\n",
    "    if(x=='Gd'):\n",
    "        r = 0\n",
    "    elif(x=='Av'):\n",
    "        r = 1\n",
    "    elif(x=='Mn'):\n",
    "        r = 2\n",
    "    elif(x=='No'):\n",
    "        r = 3\n",
    "    else:\n",
    "        r = 4\n",
    "    return r\n",
    "mydata['BsmtExposure'] = mydata['BsmtExposure'].apply(BsmtExposureToInt)\n",
    "\n",
    "def FunctionalToInt(x):\n",
    "    if(x=='Typ'):\n",
    "        r = 0\n",
    "    elif(x=='Min1'):\n",
    "        r = 1\n",
    "    elif(x=='Min2'):\n",
    "        r = 1\n",
    "    else:\n",
    "        r = 2\n",
    "    return r\n",
    "\n",
    "mydata['Functional_int'] = mydata['Functional'].apply(FunctionalToInt)\n",
    "\n",
    "\n",
    "def HouseStyleToInt(x):\n",
    "    if(x=='1.5Unf'):\n",
    "        r = 0\n",
    "    elif(x=='SFoyer'):\n",
    "        r = 1\n",
    "    elif(x=='1.5Fin'):\n",
    "        r = 2\n",
    "    elif(x=='2.5Unf'):\n",
    "        r = 3\n",
    "    elif(x=='SLvl'):\n",
    "        r = 4\n",
    "    elif(x=='1Story'):\n",
    "        r = 5\n",
    "    elif(x=='2Story'):\n",
    "        r = 6  \n",
    "    elif(x==' 2.5Fin'):\n",
    "        r = 7          \n",
    "    else:\n",
    "        r = 8\n",
    "    return r\n",
    "\n",
    "mydata['HouseStyle_int'] = mydata['HouseStyle'].apply(HouseStyleToInt)\n",
    "mydata['HouseStyle_1st'] = 1*(mydata['HouseStyle'] == '1Story')\n",
    "mydata['HouseStyle_2st'] = 1*(mydata['HouseStyle'] == '2Story')\n",
    "mydata['HouseStyle_15st'] = 1*(mydata['HouseStyle'] == '1.5Fin')\n",
    "\n",
    "def FoundationToInt(x):\n",
    "    if(x=='PConc'):\n",
    "        r = 3\n",
    "    elif(x=='CBlock'):\n",
    "        r = 2\n",
    "    elif(x=='BrkTil'):\n",
    "        r = 1        \n",
    "    else:\n",
    "        r = 0\n",
    "    return r\n",
    "\n",
    "mydata['Foundation_int'] = mydata['Foundation'].apply(FoundationToInt)\n",
    "\n",
    "def MasVnrTypeToInt(x):\n",
    "    if(x=='Stone'):\n",
    "        r = 3\n",
    "    elif(x=='BrkFace'):\n",
    "        r = 2\n",
    "    elif(x=='BrkCmn'):\n",
    "        r = 1        \n",
    "    else:\n",
    "        r = 0\n",
    "    return r\n",
    "\n",
    "mydata['MasVnrType_int'] = mydata['MasVnrType'].apply(MasVnrTypeToInt)\n",
    "\n",
    "def BsmtFinType1ToInt(x):\n",
    "    if(x=='GLQ'):\n",
    "        r = 6\n",
    "    elif(x=='ALQ'):\n",
    "        r = 5\n",
    "    elif(x=='BLQ'):\n",
    "        r = 4\n",
    "    elif(x=='Rec'):\n",
    "        r = 3   \n",
    "    elif(x=='LwQ'):\n",
    "        r = 2\n",
    "    elif(x=='Unf'):\n",
    "        r = 1        \n",
    "    else:\n",
    "        r = 0\n",
    "    return r\n",
    "\n",
    "mydata['BsmtFinType1_int'] = mydata['BsmtFinType1'].apply(BsmtFinType1ToInt)\n",
    "mydata['BsmtFinType1_Unf'] = 1*(mydata['BsmtFinType1'] == 'Unf')\n",
    "mydata['HasWoodDeck'] = (mydata['WoodDeckSF'] == 0) * 1\n",
    "mydata['HasOpenPorch'] = (mydata['OpenPorchSF'] == 0) * 1\n",
    "mydata['HasEnclosedPorch'] = (mydata['EnclosedPorch'] == 0) * 1\n",
    "mydata['Has3SsnPorch'] = (mydata['3SsnPorch'] == 0) * 1\n",
    "mydata['HasScreenPorch'] = (mydata['ScreenPorch'] == 0) * 1\n",
    "mydata['YearsSinceRemodel'] = mydata['YrSold'].astype(int) - mydata['YearRemodAdd'].astype(int)\n",
    "mydata['Total_Home_Quality'] = mydata['OverallQual'] + mydata['OverallCond']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "_cell_guid": "e35b5c7f-249b-4099-bf07-27e65eabd2e4",
    "_uuid": "5d4ff0f17c4064a73e2b1da8301d3a29eb31862b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Adding log-transformed predictors to raw data --------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "_cell_guid": "cc651c71-4b3e-4179-bab5-7324972dcdc2",
    "_uuid": "86efa6b371b871a70caa6e32e32d62d1232b0b43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addlogs(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n",
    "        res.columns.values[m] = l + '_log'\n",
    "        m += 1\n",
    "    return res\n",
    "\n",
    "loglist = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n",
    "                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n",
    "                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n",
    "                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n",
    "                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n",
    "\n",
    "mydata = addlogs(mydata, loglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "_cell_guid": "becdccd4-79f2-46a4-a3ea-9f6dc5c8de2f",
    "_uuid": "7d910865fa7c0461585735af47f9439d2eb23939",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- Creating dataset for training: adding dummies, adding numeric predictors -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_cell_guid": "575509da-a57f-4c10-96aa-c3395d852410",
    "_uuid": "356f35b9ad7eae34efe558bb1e53c2da745e8cf1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdummies(res, ls):\n",
    "    def encode(encode_df):\n",
    "        encode_df = np.array(encode_df)\n",
    "        enc = OneHotEncoder()\n",
    "        le = LabelEncoder()\n",
    "        le.fit(encode_df)\n",
    "        res1 = le.transform(encode_df).reshape(-1, 1)\n",
    "        enc.fit(res1)\n",
    "        return pd.DataFrame(enc.transform(res1).toarray()), le, enc\n",
    "\n",
    "    decoder = []\n",
    "    outres = pd.DataFrame({'A' : []})\n",
    "\n",
    "    for l in ls:\n",
    "        cat, le, enc = encode(res[l])\n",
    "        cat.columns = [l+str(x) for x in cat.columns]\n",
    "        outres.reset_index(drop=True, inplace=True)\n",
    "        outres = pd.concat([outres, cat], axis = 1)\n",
    "        decoder.append([le,enc])     \n",
    "    \n",
    "    return (outres, decoder)\n",
    "\n",
    "catpredlist = ['MSSubClass','MSZoning','LotShape','LandContour','LotConfig',\n",
    "               'Neighborhood','Condition1','Condition2','BldgType',\n",
    "               'RoofStyle','RoofMatl','Exterior1st','Exterior2nd',\n",
    "               'BsmtFinType2','Heating','HouseStyle','Foundation','MasVnrType','BsmtFinType1',\n",
    "               'Electrical','Functional','GarageType','Alley','Utilities',\n",
    "               'GarageCond','Fence','MiscFeature','SaleType','SaleCondition','LandSlope','CentralAir',\n",
    "               'GarageFinish','BsmtExposure','Street']\n",
    "\n",
    "# Applying function to get dummies\n",
    "# Saving decoder - function which can be used to transform new data  \n",
    "res = getdummies(mydata[catpredlist],catpredlist)\n",
    "df = res[0]\n",
    "decoder = res[1]\n",
    "\n",
    "# Adding real valued features\n",
    "floatpredlist = ['LotFrontage_log',\n",
    "                 'LotArea_log',\n",
    "                 'MasVnrArea_log','BsmtFinSF1_log','BsmtFinSF2_log','BsmtUnfSF_log',\n",
    "                 'TotalBsmtSF_log','1stFlrSF_log','2ndFlrSF_log','LowQualFinSF_log','GrLivArea_log',\n",
    "                 'BsmtFullBath_log','BsmtHalfBath_log','FullBath_log','HalfBath_log','BedroomAbvGr_log','KitchenAbvGr_log',\n",
    "                 'TotRmsAbvGrd_log','Fireplaces_log','GarageCars_log','GarageArea_log',\n",
    "                 'PoolArea_log','MiscVal_log',\n",
    "                 'YearRemodAdd','TotalSF_log','OverallQual','OverallCond','ExterQual','ExterCond','KitchenQual',\n",
    "                 'HeatingQC','BsmtQual','BsmtCond','FireplaceQu','GarageQual','PoolQC','PavedDrive',\n",
    "                 'HasWoodDeck', 'HasOpenPorch','HasEnclosedPorch', 'Has3SsnPorch', 'HasScreenPorch']\n",
    "df = pd.concat([df,mydata[floatpredlist]],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "_cell_guid": "54ca845d-0230-4be4-aee9-301f783d48fc",
    "_uuid": "30213f089b01ca6bca1ece4452da990a519d64d0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- Creating dataset for training: using function which creates squared predictors and adding them to the dataset -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "_cell_guid": "4070f375-7b87-4f7a-9bac-cdc11324c9b0",
    "_uuid": "effaf744a9020d6c6e0878c89e16c92e67939b04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addSquared(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n",
    "        res.columns.values[m] = l + '_sq'\n",
    "        m += 1\n",
    "    return res \n",
    "\n",
    "sqpredlist = ['YearRemodAdd', 'LotFrontage_log', \n",
    "              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n",
    "              'GarageCars_log', 'GarageArea_log',\n",
    "              'OverallQual','ExterQual','BsmtQual','GarageQual','FireplaceQu','KitchenQual']\n",
    "df = addSquared(df, sqpredlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "_cell_guid": "1e43a8b7-8b62-4170-8a25-7521e32c4151",
    "_uuid": "daf63c3110a845a9ac86f27cf896b37990b3a928",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- Converting data to numpy array ------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "_cell_guid": "cac66825-2c96-413e-8f8e-42f23d508ea3",
    "_uuid": "766c48b233ccd067b1949d91750259039854227d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(df)\n",
    "X = np.delete(X, 0, axis=1)\n",
    "y = np.log(1+np.array(mydata['SalePrice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1457, 288)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "_cell_guid": "331158e2-ec03-4baf-b5a8-71cc0b3b0ee5",
    "_uuid": "4ac7b25dc5ca27debd25e0562aa77b692c16387c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- Modelling -------------\n",
    "# 30-fold cross-validation\n",
    "# Stacking: on each run of cross-validation I fit 5 models (l2, l1, GBR, ENet and LGB)\n",
    "# Then we make 5 predictions using these models on left-out fold and add geometric mean of these predictions\n",
    "# Finally, use lasso on these six predictors to forecast values on the left-out fold\n",
    "# Save all the models (in total we have 30*6=180 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_cell_guid": "9c0c86dc-6bff-4b32-a888-a56b4e517081",
    "_uuid": "89c37bcbba705f528308355bf294f2c70040780d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  1\n",
      "fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin01258511\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  3\n",
      "fold:  4\n",
      "fold:  5\n",
      "fold:  6\n",
      "fold:  7\n",
      "fold:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin01258511\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  9\n",
      "fold:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin01258511\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  11\n",
      "fold:  12\n",
      "fold:  13\n",
      "fold:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin01258511\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  15\n",
      "fold:  16\n",
      "fold:  17\n",
      "fold:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin01258511\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  19\n",
      "fold:  20\n"
     ]
    }
   ],
   "source": [
    "nF = 20\n",
    "\n",
    "kf = KFold(n_splits=nF, random_state=241, shuffle=True)\n",
    "\n",
    "test_errors_l2 = []\n",
    "train_errors_l2 = []\n",
    "test_errors_l1 = []\n",
    "train_errors_l1 = []\n",
    "test_errors_GBR = []\n",
    "train_errors_GBR = []\n",
    "test_errors_ENet = []\n",
    "test_errors_LGB = []\n",
    "test_errors_stack = []\n",
    "test_errors_ens = []\n",
    "train_errors_ens = []\n",
    "\n",
    "models = []\n",
    "\n",
    "pred_all = []\n",
    "\n",
    "ifold = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print('fold: ',ifold)\n",
    "    ifold = ifold + 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # ridge\n",
    "    l2Regr = Ridge(alpha=9.0, fit_intercept = True)\n",
    "    l2Regr.fit(X_train, y_train)\n",
    "    pred_train_l2 = l2Regr.predict(X_train)\n",
    "    pred_test_l2 = l2Regr.predict(X_test)\n",
    "    \n",
    "    # lasso\n",
    "    l1Regr = make_pipeline(RobustScaler(), Lasso(alpha = 0.0003, random_state=1, max_iter=50000))\n",
    "    l1Regr.fit(X_train, y_train)\n",
    "    pred_train_l1 = l1Regr.predict(X_train)\n",
    "    pred_test_l1 = l1Regr.predict(X_test)\n",
    "    \n",
    "    # GBR      \n",
    "    myGBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.02,\n",
    "                                      max_depth=4, max_features='sqrt',\n",
    "                                      min_samples_leaf=15, min_samples_split=50,\n",
    "                                      loss='huber', random_state = 5) \n",
    "    \n",
    "    myGBR.fit(X_train,y_train)\n",
    "    pred_train_GBR = myGBR.predict(X_train)\n",
    "\n",
    "    pred_test_GBR = myGBR.predict(X_test)\n",
    "    \n",
    "    # ENet\n",
    "    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=4.0, l1_ratio=0.005, random_state=3))\n",
    "    ENet.fit(X_train, y_train)\n",
    "    pred_train_ENet = ENet.predict(X_train)\n",
    "    pred_test_ENet = ENet.predict(X_test) \n",
    "    \n",
    "    # LGB\n",
    "    myLGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=600,\n",
    "                              max_bin = 50, bagging_fraction = 0.6,\n",
    "                              bagging_freq = 5, feature_fraction = 0.25,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n",
    "    myLGB.fit(X_train, y_train)\n",
    "    pred_train_LGB = myLGB.predict(X_train)\n",
    "    pred_test_LGB = myLGB.predict(X_test)      \n",
    "    \n",
    "    # Stacking\n",
    "    stackedset = pd.DataFrame({'A' : []})\n",
    "    stackedset = pd.concat([stackedset,pd.DataFrame(pred_test_l2)],axis=1)\n",
    "    stackedset = pd.concat([stackedset,pd.DataFrame(pred_test_l1)],axis=1)\n",
    "    stackedset = pd.concat([stackedset,pd.DataFrame(pred_test_GBR)],axis=1)\n",
    "    stackedset = pd.concat([stackedset,pd.DataFrame(pred_test_ENet)],axis=1)\n",
    "    stackedset = pd.concat([stackedset,pd.DataFrame(pred_test_LGB)],axis=1)\n",
    "    prod = (pred_test_l2*pred_test_l1*pred_test_GBR*pred_test_ENet*pred_test_LGB) ** (1.0/5.0)\n",
    "    stackedset = pd.concat([stackedset,pd.DataFrame(prod)],axis=1)\n",
    "    Xstack = np.array(stackedset)\n",
    "    Xstack = np.delete(Xstack, 0, axis=1)\n",
    "    l1_staked = Lasso(alpha = 0.0001,fit_intercept = True)\n",
    "    l1_staked.fit(Xstack, y_test)\n",
    "    pred_test_stack = l1_staked.predict(Xstack)\n",
    "    \n",
    "    models.append([l2Regr,l1Regr,myGBR,ENet,myLGB,l1_staked])\n",
    "    \n",
    "    test_errors_l2.append(np.square(pred_test_l2 - y_test).mean() ** 0.5)\n",
    "    test_errors_l1.append(np.square(pred_test_l1 - y_test).mean() ** 0.5)\n",
    "    test_errors_GBR.append(np.square(pred_test_GBR - y_test).mean() ** 0.5)\n",
    "    test_errors_ENet.append(np.square(pred_test_ENet - y_test).mean() ** 0.5)\n",
    "    test_errors_LGB.append(np.square(pred_test_LGB - y_test).mean() ** 0.5)\n",
    "    test_errors_stack.append(np.square(pred_test_stack - y_test).mean() ** 0.5)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_cell_guid": "8407b82f-e13b-4936-99b9-71360aa13781",
    "_uuid": "a9f6ad02caade29ee8cae223ea6baa2efe4fc38a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output of test set errors; they should be lower then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_cell_guid": "9377f309-c8d4-42c4-8814-f4c46a65ba6d",
    "_uuid": "19693b8d29dca74f484c821f48fcc37b2d865cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10914198827967032\n",
      "0.10890105888453805\n",
      "0.11084442248774302\n",
      "0.21631250938730026\n",
      "0.11481912447230447\n",
      "0.09826985268731328\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_errors_l2))\n",
    "print(np.mean(test_errors_l1))\n",
    "print(np.mean(test_errors_GBR))\n",
    "print(np.mean(test_errors_ENet))\n",
    "print(np.mean(test_errors_LGB))\n",
    "print(np.mean(test_errors_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_cell_guid": "7f6c0307-6b33-464a-a183-349b7a8d9d95",
    "_uuid": "1d25a2f3237e25883d95d9b1b28dc1364af2f1d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# ----------------------- Scoring: predictions on the test set -------------------------------\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "_cell_guid": "3c2aa817-8b2c-4a5a-93f6-a30f3cdf3c67",
    "_uuid": "1796c73139b674bb9fef2b8da0b03dda75599c3d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data\n",
    "scoredata = pd.read_csv('test.csv')\n",
    "\n",
    "scoredata.to_csv('id_.csv', index = False)\n",
    "\n",
    "scoredata['MSSubClass'] = scoredata['MSSubClass'].apply(str)\n",
    "scoredata['YrSold'] = scoredata['YrSold'].astype(str)\n",
    "scoredata['MoSold'] = scoredata['MoSold'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_cell_guid": "0ade4ef9-a61d-4300-a171-13985cb22f79",
    "_uuid": "d076a9602f0bd0a783307b9f0f1181f97add3122",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- Filling in missing values, re-coding ordinal variables -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "_cell_guid": "47261029-87d6-4eff-86b4-5822cf54fc0f",
    "_uuid": "e46230a8343048152ba649c225a4549970814ecb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoredata = fill_missings(scoredata)\n",
    "\n",
    "scoredata['ExterQual'] = scoredata['ExterQual'].apply(QualToInt)\n",
    "scoredata['ExterCond'] = scoredata['ExterCond'].apply(QualToInt)\n",
    "scoredata['KitchenQual'] = scoredata['KitchenQual'].apply(QualToInt)\n",
    "scoredata['HeatingQC'] = scoredata['HeatingQC'].apply(QualToInt)\n",
    "scoredata['BsmtQual'] = scoredata['BsmtQual'].apply(QualToInt)\n",
    "scoredata['BsmtCond'] = scoredata['BsmtCond'].apply(QualToInt)\n",
    "scoredata['FireplaceQu'] = scoredata['FireplaceQu'].apply(QualToInt)\n",
    "scoredata['GarageQual'] = scoredata['GarageQual'].apply(QualToInt)\n",
    "scoredata['PoolQC'] = scoredata['PoolQC'].apply(QualToInt)\n",
    "scoredata['LandSlope'] = scoredata['LandSlope'].apply(SlopeToInt)\n",
    "scoredata['CentralAir'] = scoredata['CentralAir'].apply( lambda x: 0 if x == 'N' else 1) \n",
    "scoredata['Street'] = scoredata['Street'].apply( lambda x: 0 if x == 'Grvl' else 1) \n",
    "scoredata['GarageFinish'] = scoredata['GarageFinish'].apply(GFinishToInt)\n",
    "scoredata['BsmtExposure'] = scoredata['BsmtExposure'].apply(BsmtExposureToInt)\n",
    "\n",
    "scoredata['TotalSF'] = scoredata['TotalBsmtSF'] + scoredata['1stFlrSF'] + scoredata['2ndFlrSF']\n",
    "scoredata['TotalSF'] = scoredata['TotalSF'].fillna(0)\n",
    "\n",
    "scoredata['Functional_int'] = scoredata['Functional'].apply(FunctionalToInt)\n",
    "scoredata['HouseStyle_int'] = scoredata['HouseStyle'].apply(HouseStyleToInt)\n",
    "scoredata['HouseStyle_1st'] = 1*(scoredata['HouseStyle'] == '1Story')\n",
    "scoredata['HouseStyle_2st'] = 1*(scoredata['HouseStyle'] == '2Story')\n",
    "scoredata['HouseStyle_15st'] = 1*(scoredata['HouseStyle'] == '1.5Fin')\n",
    "scoredata['Foundation_int'] = scoredata['Foundation'].apply(FoundationToInt)\n",
    "scoredata['MasVnrType_int'] = scoredata['MasVnrType'].apply(MasVnrTypeToInt)\n",
    "scoredata['BsmtFinType1_int'] = scoredata['BsmtFinType1'].apply(BsmtFinType1ToInt)\n",
    "scoredata['BsmtFinType1_Unf'] = 1*(scoredata['BsmtFinType1'] == 'Unf')\n",
    "scoredata['PavedDrive'] = scoredata['PavedDrive'].apply( lambda x: 0 if x == 'Y' else 1)\n",
    "\n",
    "scoredata['HasWoodDeck'] = (scoredata['WoodDeckSF'] == 0) * 1\n",
    "scoredata['HasOpenPorch'] = (scoredata['OpenPorchSF'] == 0) * 1\n",
    "scoredata['HasEnclosedPorch'] = (scoredata['EnclosedPorch'] == 0) * 1\n",
    "scoredata['Has3SsnPorch'] = (scoredata['3SsnPorch'] == 0) * 1\n",
    "scoredata['HasScreenPorch'] = (scoredata['ScreenPorch'] == 0) * 1\n",
    "scoredata['Total_Home_Quality'] = scoredata['OverallQual'] + scoredata['OverallCond']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "_cell_guid": "04547886-06ff-48d6-aa4e-a271c686a200",
    "_uuid": "f9b59eb9f86605785845e0de0d67f3688c3a68b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Changing newly appeared values for some predictors --------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "_cell_guid": "1b9bbb76-fb14-4ed5-a9df-b7a23c01b5e6",
    "_uuid": "d9156303705c5a6d81731f0ccdfc68d47e441df4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scoredata['MSSubClass'] = scoredata['MSSubClass'].apply(lambda x: '20' if x == '150' else x)\n",
    "scoredata['MSZoning'] = scoredata['MSZoning'].apply(lambda x: 'RL' if x == 'missing' else x)\n",
    "scoredata['Utilities'] = scoredata['Utilities'].apply(lambda x: 'AllPub' if x == 'missing' else x)\n",
    "scoredata['Exterior1st'] = scoredata['Exterior1st'].apply(lambda x: 'VinylSd' if x == 'missing' else x)\n",
    "scoredata['Exterior2nd'] = scoredata['Exterior2nd'].apply(lambda x: 'VinylSd' if x == 'missing' else x)\n",
    "scoredata['Functional'] = scoredata['Functional'].apply(lambda x: 'Typ' if x == 'missing' else x)\n",
    "scoredata['SaleType'] = scoredata['SaleType'].apply(lambda x: 'WD' if x == 'missing' else x)\n",
    "scoredata['SaleCondition'] = scoredata['SaleCondition'].apply(lambda x: 'Normal' if x == 'missing' else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "_cell_guid": "4a46c579-bf4f-420a-b4a4-98308fbb1eb0",
    "_uuid": "794988c83e300c784e774cf4179ac8ea60070a23",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Adding log-transformed predictors to raw data --------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "_cell_guid": "12fdd7c0-912d-4232-ac4b-148c142c48ca",
    "_uuid": "eee3e2f3deacadc761f472bd2419607f6f71f21e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoredata = addlogs(scoredata, loglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "_cell_guid": "9fa65b65-e767-4b85-b630-46bd71fa0a97",
    "_uuid": "5f71d00522bb1e5dba01e37967865d44a36785b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- Creating dataset for training: dummies, adding numeric variables, adding squared predictors ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "_cell_guid": "994ed924-82fc-4d74-ab08-214ba389e8e3",
    "_uuid": "2237eb2cb82cc3b629da580cdc198a0af5838301",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdummies_transform(res, ls, decoder):\n",
    "    def encode(encode_df, le_df, enc_df):\n",
    "        encode_df = np.array(encode_df)\n",
    "        res1 = le_df.transform(encode_df).reshape(-1, 1)\n",
    "        return pd.DataFrame(enc_df.transform(res1).toarray())\n",
    "    \n",
    "    L = len(ls)\n",
    "    outres = pd.DataFrame({'A' : []})\n",
    "\n",
    "    for j in range(L):\n",
    "        l = ls[j]\n",
    "        le = decoder[j][0]\n",
    "        enc = decoder[j][1]\n",
    "        cat = encode(res[l], le, enc)\n",
    "        cat.columns = [l+str(x) for x in cat.columns]\n",
    "        outres.reset_index(drop=True, inplace=True)\n",
    "        outres = pd.concat([outres, cat], axis = 1)\n",
    "    \n",
    "    return outres\n",
    "\n",
    "df_scores = getdummies_transform(scoredata, catpredlist, decoder)\n",
    "df_scores = pd.concat([df_scores,scoredata[floatpredlist]],axis=1)\n",
    "df_scores = addSquared(df_scores, sqpredlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('test_.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "_cell_guid": "3809071d-0fc5-45c1-b3f8-371cb7757144",
    "_uuid": "23cd7f45c24a12e60cbf1680ac2a49686afd0e66",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting data into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "_cell_guid": "8f80bca4-5558-4ea5-a6f0-f1d69f85d7fa",
    "_uuid": "22983cdb71301122382adfeaf602b5f601fb0e2d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_score = np.array(df_scores)\n",
    "X_score = np.delete(X_score, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "_cell_guid": "2f84b77b-b644-4ae5-98aa-8ab58dfb4d8e",
    "_uuid": "beea2daf71d503ed8569322207d4f9b5b0666df5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "_cell_guid": "bf46e8e2-692f-419d-9533-9fe0b7d4c37d",
    "_uuid": "06a121c9245e9fcaac7d98eff251fc05bd619a68",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = X_score.shape[0]\n",
    "scores_fin = 1+np.zeros(M)\n",
    "\n",
    "for md in models:\n",
    "    l2 = md[0]\n",
    "    l1 = md[1]\n",
    "    GBR = md[2]\n",
    "    ENet = md[3]\n",
    "    LGB = md[4]\n",
    "    l1_stacked = md[5]\n",
    "    \n",
    "    l2_scores = l2.predict(X_score)\n",
    "    l1_scores = l1.predict(X_score)\n",
    "    GBR_scores = GBR.predict(X_score)\n",
    "    ENet_scores = ENet.predict(X_score)\n",
    "    LGB_scores = LGB.predict(X_score)\n",
    "    \n",
    "    stackedsets = pd.DataFrame({'A' : []})\n",
    "    stackedsets = pd.concat([stackedsets,pd.DataFrame(l2_scores)],axis=1)\n",
    "    stackedsets = pd.concat([stackedsets,pd.DataFrame(l1_scores)],axis=1)\n",
    "    stackedsets = pd.concat([stackedsets,pd.DataFrame(GBR_scores)],axis=1)\n",
    "    stackedsets = pd.concat([stackedsets,pd.DataFrame(ENet_scores)],axis=1)\n",
    "    stackedsets = pd.concat([stackedsets,pd.DataFrame(LGB_scores)],axis=1)\n",
    "    prod = (l2_scores*l1_scores*GBR_scores*ENet_scores*LGB_scores) ** (1.0/5.0)\n",
    "    stackedsets = pd.concat([stackedsets,pd.DataFrame(prod)],axis=1)    \n",
    "    Xstacks = np.array(stackedsets)\n",
    "    Xstacks = np.delete(Xstacks, 0, axis=1)\n",
    "    scores_fin = scores_fin * l1_stacked.predict(Xstacks)\n",
    "scores_fin = scores_fin ** (1/nF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "_cell_guid": "804e3b9d-4d93-4391-864b-36e37083784c",
    "_uuid": "74ee82d7973829f300271eaf90153d2888823e93",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Id = scoredata['Id']\n",
    "fin_score = np.exp((l2_scores + l1_scores + GBR_scores + LGB_scores + ENet_scores).mean())\n",
    "fin = pd.DataFrame({'Id': Id,'SalePrice': fin_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "_cell_guid": "ea9d1ff7-c7f5-4c55-b504-2e78f61f5009",
    "_uuid": "c2e29331287d9019e0c89550accaf688681366e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin.to_csv('House_Prices_submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6f9e25e9-ba58-49ae-aea1-e8be562c4b7a",
    "_uuid": "44adc01f46637aafca549fe8ee9be7ede7387dbb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0cfb489b-d01b-43ad-a850-9d18e3cb3b74",
    "_uuid": "fd191d71adae0166ae95d8cdb79edc20ac033888",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "03cdfb75-6c81-47a7-bf31-5bfd90f62e71",
    "_uuid": "c97c408554bb94d4314815fb8323281444532c2b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "463bdf2f-ed56-4bfb-9b32-995f3f3a6641",
    "_uuid": "2028d3ab544e1b924d7123d789abe0a5bd68c879",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5cec5eb-8a48-4ee0-b22a-c0dac8fb627b",
    "_uuid": "447f538fa2e5c55ba499060fc62aecc186f6bdce",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e6a698f9-6cfc-471a-a43d-d9c05709e225",
    "_uuid": "f89a6fb4d0ea15aa5372e2aa8fbd431a431a9cb6",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "688c91bd-f1c6-4354-bfd8-56c552e2d02b",
    "_uuid": "4f2f35e3bac479631e65453144b6d9c88e4974b8",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "515a46f8-f66f-4387-92ce-c32816bf149d",
    "_uuid": "f5d3222ecc260df4069367e2a18916157d550784",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c428cdfc-8e68-425c-9974-1bff1ff04656",
    "_uuid": "0c318f7db1e428002b32a4ebe91c1a257db8f0b5",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
